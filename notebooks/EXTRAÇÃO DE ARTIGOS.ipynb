{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T16:50:23.936267Z",
     "start_time": "2024-05-07T16:50:22.700610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# EXTRAÇÃO DOS ARTIGOS PUBLICADOS\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def fun_result(result):\n",
    "    \"\"\"Função auxiliar para extrair conteúdo de expressões regulares.\"\"\"\n",
    "    return result.group(1) if result is not None else 'VAZIO'\n",
    "\n",
    "\n",
    "def getperiod(zip_path, years):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as archive:\n",
    "        try:\n",
    "            with archive.open('curriculo.xml', 'r') as file:\n",
    "                content = file.read().decode('ISO-8859-1')\n",
    "        except KeyError:\n",
    "            print(f\"curriculo.xml não encontrado em {zip_path}\")\n",
    "            return pd.DataFrame()  # Retorna um DataFrame vazio em caso de falha\n",
    "\n",
    "        soup = BeautifulSoup(content, 'lxml')  # Use 'lxml' diretamente para XML parsing\n",
    "\n",
    "        dg = soup.find_all('dados-gerais')\n",
    "        if not dg:\n",
    "            print(f'Dados gerais não encontrados para {zip_path}')\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        fullname = fun_result(re.search('nome-completo=\\\"(.*)\\\" nome-em-citacoes', str(dg[0])))\n",
    "\n",
    "        pb = soup.find_all('producao-bibliografica')\n",
    "        if not pb:\n",
    "            print(f'Produções bibliográficas não encontradas para {zip_path}')\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        artspubs = pb[0].find_all('artigos-publicados')\n",
    "        if not artspubs:\n",
    "            print(f'Artigos publicados não encontrados para {zip_path}')\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        data = {\n",
    "            'TITLE': [],\n",
    "            'YEAR': [],\n",
    "            'DOI': [],\n",
    "            'LANG': [],\n",
    "            'JOURNAL': [],\n",
    "            'ISSN': [],\n",
    "            'AUTHOR': [],\n",
    "            'ORDER': [],\n",
    "            'ORDER_OK': []\n",
    "        }\n",
    "\n",
    "        for artpub in artspubs[0].find_all('artigo-publicado'):\n",
    "            dba = artpub.find_all('dados-basicos-do-artigo')\n",
    "            paperdb = str(dba[0])\n",
    "            year = fun_result(re.search('ano-do-artigo=\\\"(.*)\\\" doi', paperdb))\n",
    "            if year and year.isdigit() and int(year) in years:\n",
    "                data['TITLE'].append(fun_result(re.search('titulo-do-artigo=\\\"(.*)\\\" titulo-do-artigo-i', paperdb)))\n",
    "                data['YEAR'].append(year)\n",
    "                data['DOI'].append(fun_result(re.search('doi=\\\"(.*)\\\" flag-divulgacao-c', paperdb)))\n",
    "                data['LANG'].append(fun_result(re.search('idioma=\\\"(.*)\\\" meio-de-divulgacao=', paperdb)))\n",
    "\n",
    "                dda = artpub.find_all('detalhamento-do-artigo')\n",
    "                paperdt = str(dda[0])\n",
    "                data['JOURNAL'].append(fun_result(re.search('titulo-do-periodico-ou-revista=\\\"(.*)\\\" volume', paperdt)))\n",
    "\n",
    "                issn = fun_result(re.search('issn=\\\"(.*)\\\" local-de-public', paperdt))\n",
    "                data['ISSN'].append(issn[:4] + '-' + issn[4:] if issn != 'VAZIO' else 'VAZIO')\n",
    "\n",
    "                aut = artpub.find_all('autores')\n",
    "                authors = [fun_result(re.search('nome-completo-do-autor=\\\"(.*)\\\" nome-para-citacao', str(a))) for a in\n",
    "                           aut]\n",
    "                authororder = [fun_result(re.search('ordem-de-autoria=\\\"(.*)\\\"', str(a))) for a in aut]\n",
    "                order_ok = [order for name, order in zip(authors, authororder) if name == fullname]\n",
    "                data['AUTHOR'].append(authors)\n",
    "                data['ORDER'].append(authororder)\n",
    "                data['ORDER_OK'].append(order_ok)\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def process_zip_files(directory, years):\n",
    "    global df_all_papers\n",
    "    df_all_papers = pd.DataFrame()  # Resetando o DataFrame global para cada chamada\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".zip\"):\n",
    "            zip_path = os.path.join(directory, filename)\n",
    "            df_current = getperiod(zip_path, years)\n",
    "            df_all_papers = pd.concat([df_all_papers, df_current], ignore_index=True)\n",
    "\n",
    "    output_dir = './ARTIGOS'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    all_periods_path = f'{output_dir}/all_periods_{\",\".join(map(str, years))}.csv'\n",
    "    df_all_papers.to_csv(all_periods_path, index=False)\n",
    "    print(f'Dados de todos os currículos foram gravados em {all_periods_path}')\n",
    "\n",
    "\n",
    "# LEITURA DOS ARQUIVOS\n",
    "directory_path = '/Users/tuliorevoredo/Documents/DEV PYTHON/PLIGIA'  #SETAR O CAMINHO DO ARQUIVO COM UM OU MAIS .ZIP (OS ARQUIVOS PRECISAM ESTAR EM ZIP)\n",
    "years_of_interest = [2019, 2020, 2021, 2022, 2023, 2024]  # SEPARAR OS ANOS POR VÍRGULA, SEM LIMITES DE ANO\n",
    "process_zip_files(directory_path, years_of_interest)\n",
    "\n"
   ],
   "id": "c1df8998d764efe7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tuliorevoredo/anaconda3/lib/python3.11/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados de todos os currículos foram gravados em ./ARTIGOS/all_periods_2019,2020,2021,2022,2023,2024.csv\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
